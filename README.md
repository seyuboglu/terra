# ğŸŒ terra
A Python package that transforms free-form research workflows into reproducible pipelines. 

## ğŸš€ Getting started
Using `terra` to track your research workflow  Let's download some data to play with...
```python
from terra import Task
import pandas as pd 

@Task.make_task
def download_iris(url: str, run_dir: str = None):
    return pd.read_csv(url)

data_df = download_iris("https://raw.github...")
```
Objects returned from a `Task` are serialized and written to disk alongside some data describing the `Task`'s run (the inputs to the `Task`, the git commit, logs, etc.).   

If you inspect `data_df`, you'll notice that it's not actually a Pandas DataFrame but rather a `terra.io.Artifact`. In `terra`, an `Artifact` is simply a pointer to an object that was created by a task run and written to disk. To access the underlying DataFrame, we can just call `data_df.load()`. 

Let's move to the next step of our project: splitting the dataset into training and test splits. We'll create a second `Task` for this step: 
```python
@Task.make_task
def split_data(df: pd.DataFrame, train_frac: float=0.8, run_dir: str = None):
    
    return pd.read_csv(url) 

split_data(df=data_df, train_frac=0.7)
```
Notice how we passed `data_df`, an `Artifact`, directly to `split_data` __without__ calling `data_df.load()`. When you pass an `Artifact` to a `Task`, `terra` automatically loads it and, most importantly, records in the terra database that an `Artifact` generated by `download_iris, run_id=1` was used by `split_data, run_id=2`. 

![dag](./docs/figures/dag_figure.png)
ğŸ”‘ __Key__ â€“  By passing `data_df` to `split_data`, we have implicitly established a link between our runs of the `download_iris` and `split_data` tasks. As we add more tasks (e.g. `train_model` and `test_model`) and run them passing in the output artifacts of the earlier tasks, we build up a Directed Acyclic Graph (DAG) where nodes are `Task` runs and edges are `Artifact`s passed between those runs. This is neat because at any point in our project, we can take any `Artifact` (be it a DataFrame, np.array, torch model, or matplotlib plot) and trace the pipeline that created it all the way back to the roots of the DAG (e.g. the initial data download). 





## âš™ï¸ Installation
Clone the terra repo:
```bash
git clone https://github.com/seyuboglu/terra.git
```

Install terra in development mode:
```bash
pip install -e path/to/terra
```

Create a `terra_config.json` file for your project:
```json
{
    "storage_dir": "path/to/storage/dir",
    "slack_web_client_id": "xoxb...",
    "notify": true
}
```

Then set the `TERRA_CONFIG_PATH` variable in your environment to point to your new `terra_config.json` (you'll need to reactivate the environment): 
```bash
conda env config vars set TERRA_CONFIG_PATH="path/to/terra_config.json"
conda activate env_name
```
